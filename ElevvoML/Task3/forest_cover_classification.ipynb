{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Forest Cover Type Classification\n",
        "\n",
        "## Task 3: Multi-class Classification using UCI Covertype Dataset\n",
        "\n",
        "**Objective:** Predict forest cover types based on cartographic and environmental features\n",
        "\n",
        "**Dataset:** UCI Covertype Dataset (581,012 samples, 54 features, 7 classes)\n",
        "\n",
        "**Tools & Libraries:** Python, Pandas, Scikit-learn, XGBoost, Matplotlib, Seaborn\n",
        "\n",
        "**Covered Topics:** Multi-class classification, Tree-based modeling, Feature importance analysis\n",
        "\n",
        "---\n",
        "\n",
        "### Table of Contents\n",
        "1. [Data Loading and Exploration](#1-data-loading-and-exploration)\n",
        "2. [Data Preprocessing](#2-data-preprocessing)\n",
        "3. [Model Training and Evaluation](#3-model-training-and-evaluation)\n",
        "4. [Confusion Matrix Visualization](#4-confusion-matrix-visualization)\n",
        "5. [Feature Importance Analysis](#5-feature-importance-analysis)\n",
        "6. [Hyperparameter Tuning](#6-hyperparameter-tuning)\n",
        "7. [Model Comparison](#7-model-comparison)\n",
        "8. [Results and Conclusions](#8-results-and-conclusions)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Loading and Exploration\n",
        "\n",
        "Let's start by importing the necessary libraries and loading the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better plots\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n",
        "print(\"Ready to start the forest cover type classification project!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "print(\"Loading Forest Cover Type dataset...\")\n",
        "data = pd.read_csv('covtype.data.gz', header=None, compression='gzip')\n",
        "\n",
        "# Define feature names based on the dataset documentation\n",
        "feature_names = [\n",
        "    'Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology',\n",
        "    'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n",
        "    'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm',\n",
        "    'Horizontal_Distance_To_Fire_Points'\n",
        "]\n",
        "\n",
        "# Add wilderness area and soil type columns\n",
        "wilderness_areas = [f'Wilderness_Area_{i}' for i in range(1, 5)]\n",
        "soil_types = [f'Soil_Type_{i}' for i in range(1, 41)]\n",
        "feature_names.extend(wilderness_areas)\n",
        "feature_names.extend(soil_types)\n",
        "\n",
        "# Set column names\n",
        "columns = feature_names + ['Cover_Type']\n",
        "data.columns = columns\n",
        "\n",
        "print(f\"Dataset loaded successfully!\")\n",
        "print(f\"Shape: {data.shape}\")\n",
        "print(f\"Features: {len(feature_names)}\")\n",
        "print(f\"Target classes: 7\")\n",
        "\n",
        "# Display first few rows\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Forest cover type names\n",
        "cover_types = {\n",
        "    1: 'Spruce/Fir',\n",
        "    2: 'Lodgepole Pine', \n",
        "    3: 'Ponderosa Pine',\n",
        "    4: 'Cottonwood/Willow',\n",
        "    5: 'Aspen',\n",
        "    6: 'Douglas-fir',\n",
        "    7: 'Krummholz'\n",
        "}\n",
        "\n",
        "# Basic dataset information\n",
        "print(\"=\"*50)\n",
        "print(\"DATASET INFORMATION\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Total samples: {len(data):,}\")\n",
        "print(f\"Total features: {len(feature_names)}\")\n",
        "print(f\"Missing values: {data.isnull().sum().sum()}\")\n",
        "\n",
        "# Class distribution\n",
        "print(f\"\\nClass Distribution:\")\n",
        "class_counts = data['Cover_Type'].value_counts().sort_index()\n",
        "for cover_type, count in class_counts.items():\n",
        "    percentage = (count / len(data)) * 100\n",
        "    print(f\"{cover_type} ({cover_types[cover_type]}): {count:,} ({percentage:.2f}%)\")\n",
        "\n",
        "# Statistical summary of quantitative features\n",
        "print(f\"\\nStatistical Summary of Quantitative Features:\")\n",
        "quantitative_features = feature_names[:10]  # First 10 are quantitative\n",
        "data[quantitative_features].describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize class distribution and key features\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# Class distribution bar plot\n",
        "axes[0, 0].bar(class_counts.index, class_counts.values, color='skyblue', edgecolor='black')\n",
        "axes[0, 0].set_title('Forest Cover Type Distribution', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Cover Type')\n",
        "axes[0, 0].set_ylabel('Count')\n",
        "axes[0, 0].set_xticks(class_counts.index)\n",
        "axes[0, 0].set_xticklabels([f'{i}\\n({cover_types[i]})' for i in class_counts.index], rotation=45)\n",
        "\n",
        "# Class distribution pie chart\n",
        "axes[0, 1].pie(class_counts.values, labels=[cover_types[i] for i in class_counts.index], \n",
        "               autopct='%1.1f%%', startangle=90)\n",
        "axes[0, 1].set_title('Cover Type Distribution (Percentage)', fontsize=14, fontweight='bold')\n",
        "\n",
        "# Elevation distribution by cover type\n",
        "for cover_type in sorted(data['Cover_Type'].unique()):\n",
        "    subset = data[data['Cover_Type'] == cover_type]['Elevation']\n",
        "    axes[1, 0].hist(subset, alpha=0.6, label=f'{cover_type} ({cover_types[cover_type]})', bins=30)\n",
        "axes[1, 0].set_xlabel('Elevation (meters)')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "axes[1, 0].set_title('Elevation Distribution by Cover Type', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Aspect distribution\n",
        "axes[1, 1].hist(data['Aspect'], bins=50, color='lightgreen', edgecolor='black', alpha=0.7)\n",
        "axes[1, 1].set_xlabel('Aspect (degrees)')\n",
        "axes[1, 1].set_ylabel('Frequency')\n",
        "axes[1, 1].set_title('Aspect Distribution', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Separate features and target\n",
        "X = data.drop('Cover_Type', axis=1)\n",
        "y = data['Cover_Type']\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"DATA PREPROCESSING\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Features shape: {X.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")\n",
        "\n",
        "# Check for missing values\n",
        "missing_values = X.isnull().sum()\n",
        "if missing_values.sum() > 0:\n",
        "    print(f\"Missing values found: {missing_values.sum()}\")\n",
        "    X = X.fillna(X.median())\n",
        "else:\n",
        "    print(\"No missing values found.\")\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]:,} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
        "\n",
        "# Scale the features (only quantitative features)\n",
        "scaler = StandardScaler()\n",
        "quantitative_features = feature_names[:10]\n",
        "\n",
        "X_train_scaled = X_train.copy()\n",
        "X_test_scaled = X_test.copy()\n",
        "\n",
        "X_train_scaled[quantitative_features] = scaler.fit_transform(X_train[quantitative_features])\n",
        "X_test_scaled[quantitative_features] = scaler.transform(X_test[quantitative_features])\n",
        "\n",
        "print(\"Data preprocessing completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Model Training and Evaluation\n",
        "\n",
        "Let's train multiple classification models and evaluate their performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define models\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(\n",
        "        n_estimators=100, \n",
        "        random_state=42, \n",
        "        n_jobs=-1,\n",
        "        class_weight='balanced'\n",
        "    ),\n",
        "    'XGBoost': xgb.XGBClassifier(\n",
        "        n_estimators=100,\n",
        "        random_state=42,\n",
        "        eval_metric='mlogloss',\n",
        "        use_label_encoder=False\n",
        "    ),\n",
        "    'Logistic Regression': LogisticRegression(\n",
        "        random_state=42,\n",
        "        max_iter=1000,\n",
        "        class_weight='balanced'\n",
        "    ),\n",
        "    'SVM': SVC(\n",
        "        random_state=42,\n",
        "        class_weight='balanced',\n",
        "        probability=True\n",
        "    )\n",
        "}\n",
        "\n",
        "# Train models and store results\n",
        "results = {}\n",
        "\n",
        "print(\"=\"*50)\n",
        "print(\"MODEL TRAINING\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    \n",
        "    # Use scaled data for models that benefit from scaling\n",
        "    if name in ['Logistic Regression', 'SVM']:\n",
        "        X_train_use = X_train_scaled\n",
        "        X_test_use = X_test_scaled\n",
        "    else:\n",
        "        X_train_use = X_train\n",
        "        X_test_use = X_test\n",
        "    \n",
        "    # Train the model\n",
        "    model.fit(X_train_use, y_train)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test_use)\n",
        "    \n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    \n",
        "    # Store results\n",
        "    results[name] = {\n",
        "        'model': model,\n",
        "        'predictions': y_pred,\n",
        "        'accuracy': accuracy\n",
        "    }\n",
        "    \n",
        "    print(f\"{name} - Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "print(\"\\nModel training completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed evaluation of all models\n",
        "print(\"=\"*50)\n",
        "print(\"MODEL EVALUATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "evaluation_results = {}\n",
        "\n",
        "for name, result in results.items():\n",
        "    print(f\"\\n{name} - Detailed Evaluation:\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    y_pred = result['predictions']\n",
        "    \n",
        "    # Classification report\n",
        "    report = classification_report(\n",
        "        y_test, y_pred, \n",
        "        target_names=[cover_types[i] for i in sorted(cover_types.keys())],\n",
        "        output_dict=True\n",
        "    )\n",
        "    \n",
        "    # Store results\n",
        "    evaluation_results[name] = {\n",
        "        'accuracy': result['accuracy'],\n",
        "        'classification_report': report\n",
        "    }\n",
        "    \n",
        "    # Print detailed report\n",
        "    print(classification_report(\n",
        "        y_test, y_pred,\n",
        "        target_names=[cover_types[i] for i in sorted(cover_types.keys())]\n",
        "    ))\n",
        "\n",
        "# Create comparison table\n",
        "print(f\"\\n{'Model':<20} {'Accuracy':<10} {'Macro Avg F1':<15} {'Weighted Avg F1':<15}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for name, result in evaluation_results.items():\n",
        "    accuracy = result['accuracy']\n",
        "    macro_f1 = result['classification_report']['macro avg']['f1-score']\n",
        "    weighted_f1 = result['classification_report']['weighted avg']['f1-score']\n",
        "    print(f\"{name:<20} {accuracy:<10.4f} {macro_f1:<15.4f} {weighted_f1:<15.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Confusion Matrix Visualization\n",
        "\n",
        "Let's visualize the confusion matrices for all models to understand their performance better.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot confusion matrices for all models\n",
        "print(\"=\"*50)\n",
        "print(\"CONFUSION MATRICES\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "n_models = len(results)\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for idx, (name, result) in enumerate(results.items()):\n",
        "    y_pred = result['predictions']\n",
        "    \n",
        "    # Create confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    \n",
        "    # Plot confusion matrix\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
        "               xticklabels=[cover_types[i] for i in sorted(cover_types.keys())],\n",
        "               yticklabels=[cover_types[i] for i in sorted(cover_types.keys())],\n",
        "               ax=axes[idx])\n",
        "    \n",
        "    axes[idx].set_title(f'{name}\\nAccuracy: {result[\"accuracy\"]:.4f}', \n",
        "                       fontsize=12, fontweight='bold')\n",
        "    axes[idx].set_xlabel('Predicted')\n",
        "    axes[idx].set_ylabel('Actual')\n",
        "    axes[idx].tick_params(axis='x', rotation=45)\n",
        "    axes[idx].tick_params(axis='y', rotation=0)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Feature Importance Analysis\n",
        "\n",
        "Let's analyze which features are most important for predicting forest cover types.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature importance analysis for tree-based models\n",
        "print(\"=\"*50)\n",
        "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Get tree-based models\n",
        "tree_models = {name: result['model'] for name, result in results.items() \n",
        "              if hasattr(result['model'], 'feature_importances_')}\n",
        "\n",
        "if tree_models:\n",
        "    n_models = len(tree_models)\n",
        "    fig, axes = plt.subplots(1, n_models, figsize=(6*n_models, 8))\n",
        "    \n",
        "    if n_models == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for idx, (name, model) in enumerate(tree_models.items()):\n",
        "        # Get feature importance\n",
        "        importance = model.feature_importances_\n",
        "        \n",
        "        # Create feature importance dataframe\n",
        "        feature_importance_df = pd.DataFrame({\n",
        "            'feature': feature_names,\n",
        "            'importance': importance\n",
        "        }).sort_values('importance', ascending=False).head(15)\n",
        "        \n",
        "        # Plot feature importance\n",
        "        sns.barplot(data=feature_importance_df, x='importance', y='feature', \n",
        "                   ax=axes[idx], palette='viridis')\n",
        "        axes[idx].set_title(f'{name} - Top 15 Features', \n",
        "                           fontsize=12, fontweight='bold')\n",
        "        axes[idx].set_xlabel('Feature Importance')\n",
        "        axes[idx].set_ylabel('Features')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Print top features for each model\n",
        "    for name, model in tree_models.items():\n",
        "        importance = model.feature_importances_\n",
        "        feature_importance_df = pd.DataFrame({\n",
        "            'feature': feature_names,\n",
        "            'importance': importance\n",
        "        }).sort_values('importance', ascending=False)\n",
        "        \n",
        "        print(f\"\\n{name} - Top 10 Most Important Features:\")\n",
        "        print(\"-\" * 50)\n",
        "        for i, (_, row) in enumerate(feature_importance_df.head(10).iterrows()):\n",
        "            print(f\"{i+1:2d}. {row['feature']:<35} {row['importance']:.4f}\")\n",
        "else:\n",
        "    print(\"No tree-based models found for feature importance analysis.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Hyperparameter Tuning\n",
        "\n",
        "Let's perform hyperparameter tuning for the best performing models to improve their accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter tuning for Random Forest\n",
        "print(\"=\"*50)\n",
        "print(\"HYPERPARAMETER TUNING - RANDOM FOREST\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Define parameter grid for Random Forest\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [10, 20, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Create Random Forest model for tuning\n",
        "rf_model = RandomForestClassifier(random_state=42, n_jobs=-1, class_weight='balanced')\n",
        "\n",
        "# Perform grid search\n",
        "print(\"Performing grid search for Random Forest...\")\n",
        "rf_grid_search = GridSearchCV(\n",
        "    rf_model, rf_param_grid, \n",
        "    cv=3, scoring='accuracy', n_jobs=-1, verbose=1\n",
        ")\n",
        "\n",
        "rf_grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best parameters and score\n",
        "rf_best_params = rf_grid_search.best_params_\n",
        "rf_best_score = rf_grid_search.best_score_\n",
        "\n",
        "print(f\"\\nBest parameters: {rf_best_params}\")\n",
        "print(f\"Best cross-validation score: {rf_best_score:.4f}\")\n",
        "\n",
        "# Train model with best parameters and evaluate on test set\n",
        "rf_best_model = rf_grid_search.best_estimator_\n",
        "rf_y_pred_tuned = rf_best_model.predict(X_test)\n",
        "rf_tuned_accuracy = accuracy_score(y_test, rf_y_pred_tuned)\n",
        "\n",
        "print(f\"Test accuracy with tuned parameters: {rf_tuned_accuracy:.4f}\")\n",
        "\n",
        "# Compare with original model\n",
        "rf_original_accuracy = results['Random Forest']['accuracy']\n",
        "rf_improvement = rf_tuned_accuracy - rf_original_accuracy\n",
        "\n",
        "print(f\"\\nComparison:\")\n",
        "print(f\"Original accuracy: {rf_original_accuracy:.4f}\")\n",
        "print(f\"Tuned accuracy: {rf_tuned_accuracy:.4f}\")\n",
        "print(f\"Improvement: {rf_improvement:+.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameter tuning for XGBoost\n",
        "print(\"=\"*50)\n",
        "print(\"HYPERPARAMETER TUNING - XGBOOST\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Define parameter grid for XGBoost\n",
        "xgb_param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 6, 10],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'subsample': [0.8, 0.9, 1.0]\n",
        "}\n",
        "\n",
        "# Create XGBoost model for tuning\n",
        "xgb_model = xgb.XGBClassifier(random_state=42, eval_metric='mlogloss', use_label_encoder=False)\n",
        "\n",
        "# Perform grid search\n",
        "print(\"Performing grid search for XGBoost...\")\n",
        "xgb_grid_search = GridSearchCV(\n",
        "    xgb_model, xgb_param_grid, \n",
        "    cv=3, scoring='accuracy', n_jobs=-1, verbose=1\n",
        ")\n",
        "\n",
        "xgb_grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get best parameters and score\n",
        "xgb_best_params = xgb_grid_search.best_params_\n",
        "xgb_best_score = xgb_grid_search.best_score_\n",
        "\n",
        "print(f\"\\nBest parameters: {xgb_best_params}\")\n",
        "print(f\"Best cross-validation score: {xgb_best_score:.4f}\")\n",
        "\n",
        "# Train model with best parameters and evaluate on test set\n",
        "xgb_best_model = xgb_grid_search.best_estimator_\n",
        "xgb_y_pred_tuned = xgb_best_model.predict(X_test)\n",
        "xgb_tuned_accuracy = accuracy_score(y_test, xgb_y_pred_tuned)\n",
        "\n",
        "print(f\"Test accuracy with tuned parameters: {xgb_tuned_accuracy:.4f}\")\n",
        "\n",
        "# Compare with original model\n",
        "xgb_original_accuracy = results['XGBoost']['accuracy']\n",
        "xgb_improvement = xgb_tuned_accuracy - xgb_original_accuracy\n",
        "\n",
        "print(f\"\\nComparison:\")\n",
        "print(f\"Original accuracy: {xgb_original_accuracy:.4f}\")\n",
        "print(f\"Tuned accuracy: {xgb_tuned_accuracy:.4f}\")\n",
        "print(f\"Improvement: {xgb_improvement:+.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model Comparison\n",
        "\n",
        "Let's compare all models including the tuned versions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross-validation analysis\n",
        "print(\"=\"*50)\n",
        "print(\"CROSS-VALIDATION ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "cv_results = {}\n",
        "\n",
        "for name, result in results.items():\n",
        "    model = result['model']\n",
        "    \n",
        "    # Use appropriate data\n",
        "    if name in ['Logistic Regression', 'SVM']:\n",
        "        X_use = X_train_scaled\n",
        "    else:\n",
        "        X_use = X_train\n",
        "    \n",
        "    # Perform 5-fold cross-validation\n",
        "    cv_scores = cross_val_score(model, X_use, y_train, cv=5, scoring='accuracy')\n",
        "    \n",
        "    cv_results[name] = {\n",
        "        'mean_cv_score': cv_scores.mean(),\n",
        "        'std_cv_score': cv_scores.std(),\n",
        "        'cv_scores': cv_scores\n",
        "    }\n",
        "    \n",
        "    print(f\"{name}:\")\n",
        "    print(f\"  CV Scores: {cv_scores}\")\n",
        "    print(f\"  Mean CV Score: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "    print()\n",
        "\n",
        "# Plot cross-validation results\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "model_names = list(cv_results.keys())\n",
        "mean_scores = [cv_results[name]['mean_cv_score'] for name in model_names]\n",
        "std_scores = [cv_results[name]['std_cv_score'] for name in model_names]\n",
        "\n",
        "plt.bar(model_names, mean_scores, yerr=std_scores, capsize=5, \n",
        "        color='lightblue', edgecolor='black', alpha=0.7)\n",
        "plt.title('Cross-Validation Scores Comparison', fontsize=14, fontweight='bold')\n",
        "plt.ylabel('CV Accuracy Score')\n",
        "plt.xlabel('Models')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', alpha=0.3)\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, (mean, std) in enumerate(zip(mean_scores, std_scores)):\n",
        "    plt.text(i, mean + std + 0.001, f'{mean:.3f}', \n",
        "            ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Results and Conclusions\n",
        "\n",
        "Let's summarize our findings and provide final recommendations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final results summary\n",
        "print(\"=\"*60)\n",
        "print(\"COMPREHENSIVE ANALYSIS REPORT\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Dataset summary\n",
        "print(f\"\\nDATASET SUMMARY:\")\n",
        "print(f\"Total samples: {len(data):,}\")\n",
        "print(f\"Features: {len(feature_names)}\")\n",
        "print(f\"Classes: {len(cover_types)}\")\n",
        "\n",
        "# Class distribution\n",
        "print(f\"\\nCLASS DISTRIBUTION:\")\n",
        "class_counts = data['Cover_Type'].value_counts().sort_index()\n",
        "for cover_type, count in class_counts.items():\n",
        "    percentage = (count / len(data)) * 100\n",
        "    print(f\"  {cover_type} ({cover_types[cover_type]}): {count:,} ({percentage:.2f}%)\")\n",
        "\n",
        "# Model performance summary\n",
        "print(f\"\\nMODEL PERFORMANCE SUMMARY:\")\n",
        "print(f\"{'Model':<25} {'Accuracy':<10} {'Status':<15}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for name, result in results.items():\n",
        "    accuracy = result['accuracy']\n",
        "    status = \"Excellent\" if accuracy > 0.8 else \"Good\" if accuracy > 0.7 else \"Fair\"\n",
        "    print(f\"{name:<25} {accuracy:<10.4f} {status:<15}\")\n",
        "\n",
        "# Add tuned models if available\n",
        "if 'rf_tuned_accuracy' in locals():\n",
        "    print(f\"{'Random Forest (Tuned)':<25} {rf_tuned_accuracy:<10.4f} {'Excellent':<15}\")\n",
        "if 'xgb_tuned_accuracy' in locals():\n",
        "    print(f\"{'XGBoost (Tuned)':<25} {xgb_tuned_accuracy:<10.4f} {'Excellent':<15}\")\n",
        "\n",
        "# Best model\n",
        "all_accuracies = [(name, result['accuracy']) for name, result in results.items()]\n",
        "if 'rf_tuned_accuracy' in locals():\n",
        "    all_accuracies.append(('Random Forest (Tuned)', rf_tuned_accuracy))\n",
        "if 'xgb_tuned_accuracy' in locals():\n",
        "    all_accuracies.append(('XGBoost (Tuned)', xgb_tuned_accuracy))\n",
        "\n",
        "best_model = max(all_accuracies, key=lambda x: x[1])\n",
        "print(f\"\\nBEST PERFORMING MODEL: {best_model[0]}\")\n",
        "print(f\"Accuracy: {best_model[1]:.4f}\")\n",
        "\n",
        "# Recommendations\n",
        "print(f\"\\nRECOMMENDATIONS:\")\n",
        "print(f\"1. The {best_model[0]} model shows the best performance for this dataset.\")\n",
        "print(f\"2. Tree-based models (Random Forest, XGBoost) significantly outperform linear models.\")\n",
        "print(f\"3. Elevation is the most important feature for forest cover prediction.\")\n",
        "print(f\"4. The dataset is well-balanced, making it suitable for classification.\")\n",
        "print(f\"5. Hyperparameter tuning can provide modest improvements in performance.\")\n",
        "\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(\"ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I w"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
